<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">
    <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
    <!-- Replace the content tag with appropriate information -->
    <meta name="description" content="DESCRIPTION META TAG">
    <meta property="og:title" content="SOCIAL MEDIA TITLE TAG" />
    <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG" />
    <meta property="og:url" content="URL OF THE WEBSITE" />
    <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
    <meta property="og:image" content="static/image/your_banner_image.png" />
    <meta property="og:image:width" content="1200" />
    <meta property="og:image:height" content="630" />


    <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
    <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
    <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
    <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
    <meta name="twitter:card" content="summary_large_image">
    <!-- Keywords for your paper to be indexed by-->
    <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
    <meta name="viewport" content="width=device-width, initial-scale=1">


    <title>EgoLife Blog</title>
    <link rel="icon" type="image/x-icon" href="static/images/egolife_circle.ico">
    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

    <link rel="stylesheet" href="static/css/bulma.min.css">
    <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="static/css/index.css">
    <link href="css/fontawesome-all.css" rel="stylesheet">
    <link href="css/font-awesome.css" rel="stylesheet">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
    <script defer src="static/js/fontawesome.all.min.js"></script>
    <script src="static/js/bulma-carousel.min.js"></script>
    <script src="static/js/bulma-slider.min.js"></script>
    <script src="static/js/index.js"></script>

    <script src="js/jquery.countup.js"></script>
    <script>
        $('.counter').countUp();
    </script>
</head>

<body>


<section class="hero">
    <div class="hero-body">
        <div class="container is-max-desktop">
            <div class="columns is-centered">
                <div class="column has-text-centered">
                    <h1 class="title is-1 publication-title"><img src="static/images/logo_square.ico" style="width: 20%; height: 20%"/><br>EgoLife: Towards Egocentric Life Assistant</h1>
                    <div class="is-size-5 publication-authors">
                        <!-- Paper authors -->
                        <span class="author-block">
                            <a href="https://jingkang50.github.io/" target="_blank">Jingkang Yang</a>,</span>
                        <span class="author-block">
                            <a href="https://choiszt.github.io/" target="_blank">Shuai Liu</a>,</span>
                        <span class="author-block">
                            <a href="https://egolife-ntu.github.io/" target="_blank">Hongming Guo</a>,</span>
                        <span class="author-block">
                            <a href="https://scholar.google.com/citations?user=kMui170AAAAJ&hl=zh-CN" target="_blank">Yuhao Dong</a>,</span>
                        <span class="author-block">
                            <a href="https://www.researchgate.net/scientific-contributions/Xiamengwei-Zhang-2298207971" target="_blank">Xiamengwei Zhang</a>,</span><br>
                        <span class="author-block">
                            <a href="https://fesvhtr.github.io/zsc/" target="_blank">Sicheng Zhang</a>,</span>
                        <span class="author-block">
                            <a href="https://openreview.net/profile?id=~Pengyun_Wang2" target="_blank">Pengyun Wang</a>,</span>
                        <span class="author-block">
                            <a href="https://scholar.google.com/citations?user=c35sPU8AAAAJ&hl=en" target="_blank">Zitang Zhou</a>,</span>
                        <span class="author-block">
                            <a href="https://nicous20.github.io/" target="_blank">Binzhu Xie</a>,</span>
                        <span class="author-block">
                            <a href="https://scholar.google.com/citations?user=CSsCwyAAAAAJ&hl=zh-CN&authuser=2" target="_blank">Ziyue Wang</a>,</span>
                        <span class="author-block">
                            <a href="https://networks.imdea.org/team/imdea-networks-team/people/bei-ouyang/" target="_blank">Bei Ouyang</a>,</span><br>
                        <span class="author-block">
                            <a href="https://scholar.google.com/citations?user=XbFxzicAAAAJ&hl=en" target="_blank">Zhengyu Lin</a>,</span>
                        <span class="author-block">
                            <a href="https://www.marcocominelli.com/" target="_blank">Marco Cominelli</a>,</span>
                        <span class="author-block">
                            <a href="https://caizhongang.com/" target="_blank">Zhongang Cai</a>,</span>
                        <span class="author-block">
                            <a href="https://zhangyuanhan-ai.github.io/" target="_blank">Yuanhan Zhang</a>,</span>
                        <span class="author-block">
                            <a href="https://veiled-texture-20c.notion.site/Perry-Peiyuan-Zhang-ab24b48621c9491db767a76df860873a" target="_blank">Peiyuan Zhang</a>,</span><br>
                        <span class="author-block">
                            <a href="https://hongfz16.github.io/" target="_blank">Fangzhou Hong</a>,</span>
                        <span class="author-block">
                            <a href="https://networks.imdea.org/team/imdea-networks-team/people/joerg-widmer/" target="_blank">Joerg Widmer</a>,</span>
                        <span class="author-block">
                            <a href="https://ans.unibs.it/people/gringoli/" target="_blank">Francesco Gringoli</a>,</span>
                        <span class="author-block">
                            <a href="https://scholar.google.com.hk/citations?user=jZH2IPYAAAAJ&hl=en" target="_blank">Lei Yang</a>,</span>
                        <span class="author-block">
                            <a href="https://brianboli.com/" target="_blank">Bo Li</a>,</span>
                        <span class="author-block">
                            <a href="https://liuziwei7.github.io/" target="_blank">Ziwei Liu</a></span>
                    </div>

                    <div class="is-size-5 publication-authors">
                        <span class="author-block"><sup>1</sup>S-Lab, Nanyang Technological University, Singapore &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <sup>2</sup>LMMs-Lab<br> <sup>3</sup>SenseTime Research &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <sup>4</sup>IMDEA Networks, Spain &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <sup>5</sup>University of Brescia, Italy</span>
                    </div>

                    <div class="column has-text-centered">
                        <div class="publication-links">
                            <!-- ArXiv abstract Link -->
                            <span class="link-block">
                  <a href="https://arxiv.org/" target="_blank"
                     class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                                <span>arXiv</span>
                                </a>
                                </span>
                            <!-- Dataset -->
                            <span class="link-block">
                  <a href="https://huggingface.co/datasets/EgoLife" target="_blank"
                     class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <p style="font-size:20px">&#x1F917;</p>
                  </span>
                  <span>Dataset</span>
                </a>
              </span>

                            <!-- Github link -->
                            <span class="link-block">
                    <a href="https://github.com/LMMs-Lab/EgoLife" target="_blank"
                       class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                                <span>Code</span>
                                </a>
                                </span>

                            <!-- Video link-->
                            <span class="link-block">
                  <a href="https://youtu.be/" target="_blank"
                     class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <p style="font-size:20px">&#127902;</p>
                  </span>
                  <span>EgoLife Short Trailer</span>
                </a>
              </span>


                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>
</section>


<!-- add a video carousel here -->


<!-- Paper abstract -->
<section class="section hero is-light">
    <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
            <div class="column is-four-fifths">
                <h2 class="title is-3">Abstract</h2>
                <div class="content has-text-justified">
                    <p>
                        We introduce <font color="#2E86C1"><b>EgoLife</b></font>, a project to develop an egocentric life assistant that accompanies and enhances personal efficiency through AI-powered wearable glasses 👓. To lay the foundation for this assistant, we conducted a comprehensive data collection study where six participants lived together for one week, continuously recording their daily activities—including discussions 💬, shopping 🛍️, cooking 🍳, socializing 👥, and entertainment 🎮—using AI glasses for multimodal egocentric video capture, along with synchronized third-person-view video references. This effort resulted in the <font color="#2E86C1"><b>EgoLife Dataset</b></font> 📖, a comprehensive 300-hour egocentric, interpersonal, multiview, and multimodal daily life dataset with intensive annotation. Leveraging this dataset, we introduce <font color="#E74C3C"><b>EgoLifeQA</b></font>❓, a suite of 6K long-context, life-oriented question-answering tasks designed to provide meaningful assistance in daily life by addressing practical questions such as recalling past relevant events, monitoring health habits, and offering personalized recommendations.
                        <br><br>
                        To address the key technical challenges of <font color="#27AE60">1)</font> developing robust visual-audio models for egocentric data, <font color="#27AE60">2)</font> enabling identity recognition, and <font color="#27AE60">3)</font> facilitating long-context question answering over extensive temporal information, we introduce <font color="#8E44AD"><b>EgoBulter</b></font> 🫡, an integrated system comprising <font color="#E67E22"><b>EgoGPT</b></font> 🧠 and <font color="#E67E22"><b>EgoRAG</b></font> 🔍. EgoGPT is a vision-language model trained on egocentric datasets, achieving state-of-the-art performance on egocentric video understanding. EgoRAG is a retrieval-based component that supports answering ultra-long-context questions. Our experimental studies verify their working mechanisms and reveal critical factors and bottlenecks, guiding future improvements. By releasing our datasets, models, and benchmarks, we aim to stimulate further research in egocentric AI assistants.
                    </p>
                </div>
            </div>
        </div>
    </div>
</section>
<!-- End paper abstract -->


<section class="hero is-small">
    <div class="hero-body">
        <div class="container">
        <h2 class="title is-2" style="text-align: center;">The EgoLife Dataset</h2>
        <h2 class="title is-3" style="text-align: center;">
            <font color="#2E86C1"><b>The EgoLife dataset</b></font> captures the lives of six participants over seven days,<br>
            recorded through <font color="#8E44AD">Meta Aria glasses</font>.<br>
            Living together with a shared goal of organizing an <font color="#27AE60">Earth Day Event</font>,<br>
            their experiences are preserved in over <font color="#E67E22"><b>250 hours</b></font> of rich, everyday data.<br>
            This includes <span style="color:#808080">egocentric</span>, <span style="color:#808080">interpersonal</span> and <span style="color:#808080">multiview</span> moments,<br>
            offering a detailed and intimate view of their lives and teamwork.
        </h2>
        <div class="columns is-centered">
            <div class="column is-four-fifths">
                <div style="width: 100%; margin: 0 auto;">
                    <video width="100%" poster="" id="tree" autoplay controls muted loop>
                        <source src="static/videos/trailer_0224_compress.mp4" type="video/mp4">
                    </video>
                </div>
            </div>
        </div>
    </div>
</section>


<!-- Video carousel -->
<section class="hero is-small is-light">
    <div class="hero-body">
        <div class="container">
            <h2 class="title is-2" style="text-align: center;">What EgoLife Captures and Annotates</h2>
            <h2 class="title is-3" style="text-align: center;">
                <font color="#2E86C1"><b>The EgoLife dataset</b></font> captures the lives of six participants over seven days,<br>
                recorded through <font color="#8E44AD">Meta Aria glasses</font>.<br>
                Living together with a shared goal of organizing an <font color="#27AE60">Earth Day Event</font>,<br>
                their experiences are preserved in over <font color="#E67E22"><b>250 hours</b></font> of rich, everyday data.<br>
                This includes <span style="color:#808080">egocentric</span>, <span style="color:#808080">interpersonal</span> and <span style="color:#808080">multiview</span> moments,<br>
                offering a detailed and intimate view of their lives and teamwork.
            </h2>
            <div id="results-carousel" class="carousel results-carousel">
                <div class="item0">
                    <h3 class="title is-4" style="text-align: center;">Multimodal Data Collection</h3>
                    <div style="min-width: 300px;
                        margin-right: 20px;
                        text-align: center;">
                        <img src="static/images/egolife_teaser.png" width="90%" height="100%" style="margin: 0 auto;">
                        <h2 class="subtitle" style="margin-top: 20px; font-size: 16px; color: #666; text-align: justify; margin-left: 25px; margin-right: 25px;">
                            The EgoLife dataset contains rich multimodal signals including first-person video and audio recordings, eye gaze tracking data, IMU motion sensors, GPS location information, and environmental audio, providing a comprehensive view of daily activities.
                        </h2>
                    </div>
                </div>
                <div class="item0">
                    <h3 class="title is-4" style="text-align: center;">Living Environment</h3>
                    <div style="min-width: 300px;
                        margin-right: 20px;
                        text-align: center;">
                        <img src="static/images/egohouse.png" width="60%" height="60%" style="margin: 0 auto;">
                        <h2 class="subtitle" style="margin-top: 20px; font-size: 16px; color: #666;text-align: justify; margin-left: 25px; margin-right: 25px;">
                        The image shows a 3D reconstruction of the shared living space created using Aria Multi-MPS. The house also features 15 Exo cameras installed throughout the common areas and 2 mmWave devices (marked in red) positioned on the second floor. The visualization also includes color-coded traces showing 10-minute movement patterns of the participants throughout the house.
                        </h2>
                    </div>
                </div>
                <div class="item0">
                    <h3 class="title is-4" style="text-align: center;">Time Synchronization</h3>
                    <div style="min-width: 300px;
                        margin-right: 20px;
                        text-align: center;">
                        <video width="75%" poster="" id="tree" autoplay controls muted loop height="100%">
                            <source src="static/videos/timesync.mp4"
                                    type="video/mp4">
                        </video>
                        <h2 class="subtitle" style="margin-top: 20px; font-size: 16px; color: #666;text-align: justify; margin-left: 25px; margin-right: 25px;">
                        The video shows the time synchronization of the egocentric and third-person views. The synchronization is achieved by aligning the audio signals of each group (e.g., egocentric, level-1, level-2) first then across views with automatic audio alignment and human-in-the-loop.
                        </h2>
                    </div>
                </div>
                <div class="item0">
                    <h3 class="title is-4" style="text-align: center;">Activity Timeline</h3>
                    <div style="min-width: 300px;
                        margin-right: 20px;
                        text-align: center;">
                        <img src="static/images/timeline.png" width="90%" height="100%" style="margin: 0 auto;">
                        <h2 class="subtitle" style="margin-top: 20px; font-size: 16px; color: #666; text-align: justify; margin-left: 25px; margin-right: 25px;">
                        The timeline provides a comprehensive overview of each participant's activities across the seven days, showing how they spent their time individually and collaboratively. From daily routines to team meetings to entertainment and event preparations, it offers a clear visualization of the participants' journey throughout the study period. The figure is inspired by Github contribution graph and is implemented by python matplotlib.
                        </h2>
                    </div>
                </div>
                <div class="item">
                    <div style="min-width: 300px;
                      margin-right: 5px;">
                        <video width="100%" poster="" id="tree" autoplay controls muted loop height="100%">
                            <!-- Your video here -->
                            <source src="static/videos/DAY3_A6_SHURE_14553000.mp4"
                                    type="video/mp4">
                        </video>
                    </div>
                    <div class="demo-questions">
                        <div style="width: 100%;">
                            <p style="font-size: 18px;
                          line-height: 20px;
                          margin: 5px 0;
                          font-weight: 400;
                          color: #161616;">
                          </p>
                        </div>
                    </div>
                    <div class="demo-questions">
                        <div style="width: 100%;">
                            <p style="font-size: 18px;
                          line-height: 20px;
                          margin: 5px 0;
                          font-weight: 400;
                          color: #161616;">
                          </p>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>
</section>
<!-- End image carousel -->

<!-- Teaser video-->
<!--
<section class="hero teaser">
    <div class="container is-max-desktop">
        <div class="hero-body">
            <video poster="" id="tree" autoplay controls muted loop height="100%">

    <source src="static/videos/banner_video.mp4"
    type="video/mp4">
  </video>
            <h2 class="subtitle has-text-centered">
                Aliquam vitae elit ullamcorper tellus egestas pellentesque. Ut lacus tellus, maximus vel lectus at, placerat pretium mi. Maecenas dignissim tincidunt vestibulum. Sed consequat hendrerit nisl ut maximus.
            </h2>
        </div>
    </div>
</section>-->
<!-- End teaser video -->




<!-- Image carousel -->
<section class="hero is-small">
    <div class="hero-body">
        <div class="container">
            <h2 class="title is-2" style="text-align: center;">EgoLifeQA: Challenges for Personal Life Assistant</h2>
            <p style="text-align: left; font-size: 20px; line-height: 1.2; margin: 20px 100px; text-align: justify;">
                <span style="font-size: 20px; font-weight: bold;">Based on the storyline of EgoLife dataset, we created EgoLifeQA with 3K life-oriented questions requiring ultra-long context understanding. We ensure that 75% of questions need looking back over 2 hours of history and 28% require reviewing more than 24 hours of past activities. 
                We specifically design the following 5 types of QAs to evaluate the performance of the life assistant. 
            </p>
            <div id="results-carousel1" class="carousel results-carousel1">
                <div class="item">
                    <div style="min-width: 300px;
                      margin-right: 20px;
                      margin-left: 20px;
                      text-align: center;">
                        <h2 class="subtitle" style="margin-bottom: 20px; font-size: 24px; text-align: justify;">
                            Question Types and Examples in the EgoLifeQA Benchmark. Each example includes a multiple-choice Q&A with supporting evidence from timestamps at least 5 minutes prior to the question. Black vertical lines indicate question timestamps, while colored curved lines connect to relevant evidence timestamps.
                        </h2>
                        <img src="static/images/egolifeqa.png" alt="MY ALT TEXT" />
                    </div>
                </div>
                <div class="item">
                    <div style="min-width: 300px;
                      margin-right: 20px;
                      margin-left: 20px;
                      text-align: center;">
                        <h2 class="subtitle" style="margin-bottom: 20px; font-size: 24px;">
                            <font color=#f44a5e><b>EntityLog</b></font> for tracking object details, usage and status history<br>
                            <i style="font-size: 20px;">e.g., "What is the price of the yogurt?"</i>
                        </h2>
                        <video width="100%" poster="" id="tree" autoplay controls muted loop height="100%">
                            <source src="static/videos/实体日志片段.mp4" type="video/mp4">
                        </video>
                    </div>
                </div>
                <div class="item">
                    <div style="min-width: 300px;
                      margin-right: 20px;
                      margin-left: 20px;
                      text-align: center;">
                        <h2 class="subtitle" style="margin-bottom: 20px; font-size: 24px;">
                            <font color=#f17338><b>EventRecall</b></font> for remembering past activities<br>
                            <i style="font-size: 20px;">e.g., "What did I eat for lunch this week?"</i>
                        </h2>
                        <video width="100%" poster="" id="tree" autoplay controls muted loop height="100%">
                            <source src="static/videos/事件回忆片段.mp4" type="video/mp4">
                        </video>
                    </div>
                </div>
                <div class="item">
                    <div style="min-width: 300px;
                      margin-right: 20px;
                      margin-left: 20px;
                      text-align: center;">
                        <h2 class="subtitle" style="margin-bottom: 20px; font-size: 24px;">
                            <font color=#f44a5e><b>TaskMaster</b></font> for intelligent task management<br>
                            <i style="font-size: 20px;">e.g., "What else should I buy? I remember I used out of something..."</i>
                        </h2>
                        <video width="100%" poster="" id="tree" autoplay controls muted loop height="100%">
                            <source src="static/videos/任务管理片段.mp4" type="video/mp4">
                        </video>
                    </div>
                </div>
                <div class="item">
                    <div style="min-width: 300px;
                      margin-right: 20px;
                      margin-left: 20px;
                      text-align: center;">
                        <h2 class="subtitle" style="margin-bottom: 20px; font-size: 24px;">
                            <font color=#f17338><b>RelationMap</b></font> for understanding social interactions<br>
                            <i style="font-size: 20px;">e.g., "Who was missing from the party?"</i>
                        </h2>
                        <video width="100%" poster="" id="tree" autoplay controls muted loop height="100%">
                            <source src="static/videos/关系网络片段.mp4" type="video/mp4">
                        </video>
                    </div>
                </div>
                <div class="item">
                    <div style="min-width: 300px;
                      margin-right: 20px;
                      margin-left: 20px;
                      text-align: center;">
                        <h2 class="subtitle" style="margin-bottom: 20px; font-size: 24px;">
                            <font color=#f44a5e><b>HabitInsight</b></font> for analyzing personal patterns<br>
                            <i style="font-size: 20px;">e.g., "What is my favorite vegetable? Which fruit I eat most these days?"</i>
                        </h2>
                        <video width="100%" poster="" id="tree" autoplay controls muted loop height="100%">
                            <source src="static/videos/习惯洞察片段.mp4" type="video/mp4">
                        </video>
                    </div>
                </div>
            </div>
        </div>
    </div>
</section>
<!-- End image carousel -->



<!-- Image carousel -->
<section class="hero is-small is-light">
    <div class="hero-body">
        <div class="container">
            <h2 class="title is-2" style="text-align: center;">EgoGPT & EgoRAG for AI Assistants</h2>
            <div id="results-carousel1" class="carousel results-carousel1">
                <div class="item">
                    <!-- Your image here -->
                    <h2 class="title is-3" style="text-align: center;">Comparison between different benchmarks</h2>
                    <h2 class="subtitle">
                        <br>Compare to other datasets, FunQA revolves around the captivating realm of interesting and counter-intuitive videos. The tasks within FunQA are specifically designed to challenge the vision capabilities of models, requiring
                        strong skills in producing an in-depth description, interpretation, and spatial-temporal reasoning. Here we clarify the abbreviation in the table. 
                        <b>Avg Len</b>b> denotes video average length; <b># Clips</b>b> means number of video clips; <b>VC</b> for visual-centric, <b>Des.</b> for Description, <b>Exp.</b> for Explanation, 
                        <b>STR</b> for Spatial-temporal Reasoning, <b>MC</b> means Multiple Choice QA, and <b>OE</b> shows Open Ended QA with <b>Average Word Count</b> per response.
                    </h2>
                    <img src="static/images/egobulter.png" alt="MY ALT TEXT" />

                </div>
                <div class="item">
                    <!-- Your image here -->
                    <h2 class="title is-3" style="text-align: center;">Comparison of responses from different models</h2>
                    <h2 class="subtitle">
                        <br>Here shows the answers given by VideoChat, Video-ChatGPT, and Otter on HumorQA video. On task H2, H3, VideoChat has the best performance. On task H4, Video-ChatGPT and Otter answer better, which is in line with our experiment
                        result. However, the answers from all models are still far from the ground truth. The descriptions of details and counter-intuitive explanations have numerous shortcomings. For example, Video-ChatGPT added incorrect details
                        to the description, such as "wearing sunglasses", the humorous reason for "throwing ketchup" was wrongly interpreted by VideoChat as "knocking over the ketchup bottle", etc.
                    </h2>
                    <img src="static/images/egogpt_results.png" alt="MY ALT TEXT" />
                </div>
                <div class="item">
                    <!-- Your image here -->
                    <h2 class="title is-3" style="text-align: center;">Comparison of responses from different models</h2>
                    <h2 class="subtitle">
                        <br>Here shows the answers given by VideoChat, Video-ChatGPT, and Otter on HumorQA video. On task H2, H3, VideoChat has the best performance. On task H4, Video-ChatGPT and Otter answer better, which is in line with our experiment
                        result. However, the answers from all models are still far from the ground truth. The descriptions of details and counter-intuitive explanations have numerous shortcomings. For example, Video-ChatGPT added incorrect details
                        to the description, such as "wearing sunglasses", the humorous reason for "throwing ketchup" was wrongly interpreted by VideoChat as "knocking over the ketchup bottle", etc.
                    </h2>
                    <img src="static/images/qualitative.png" alt="MY ALT TEXT" />
                </div>
            </div>
        </div>
    </div>
</section>
<!-- End image carousel -->


<section class="hero is-small">
    <div class="hero-body">
        <div class="container">
            <h2 class="title is-2" style="text-align: center;">Other Interesting Info</h2>
            <div id="results-carousel1" class="carousel results-carousel1">
                <div class="item">
                    <!-- Your image here -->
                    <h2 class="subtitle">
                        <br>Compare to other datasets, FunQA revolves around the captivating realm of interesting and counter-intuitive videos. The tasks within FunQA are specifically designed to challenge the vision capabilities of models, requiring
                        strong skills in producing an in-depth description, interpretation, and spatial-temporal reasoning. Here we clarify the abbreviation in the table. 
                        <b>Avg Len</b>b> denotes video average length; <b># Clips</b>b> means number of video clips; <b>VC</b> for visual-centric, <b>Des.</b> for Description, <b>Exp.</b> for Explanation, 
                        <b>STR</b> for Spatial-temporal Reasoning, <b>MC</b> means Multiple Choice QA, and <b>OE</b> shows Open Ended QA with <b>Average Word Count</b> per response.
                    </h2>
                    <img src="static/images/egobulter.png" alt="MY ALT TEXT" />

                </div>
                <div class="item">
                    <!-- Your image here -->
                    <h2 class="title is-3" style="text-align: center;">Comparison of responses from different models</h2>
                    <h2 class="subtitle">
                        <br>Here shows the answers given by VideoChat, Video-ChatGPT, and Otter on HumorQA video. On task H2, H3, VideoChat has the best performance. On task H4, Video-ChatGPT and Otter answer better, which is in line with our experiment
                        result. However, the answers from all models are still far from the ground truth. The descriptions of details and counter-intuitive explanations have numerous shortcomings. For example, Video-ChatGPT added incorrect details
                        to the description, such as "wearing sunglasses", the humorous reason for "throwing ketchup" was wrongly interpreted by VideoChat as "knocking over the ketchup bottle", etc.
                    </h2>
                    <img src="static/images/egogpt_results.png" alt="MY ALT TEXT" />
                </div>
            </div>
        </div>
    </div>
</section>

<section class="hero is-small is-light">
    <div class="hero-body">
        <div class="container">
            <h2 class="title is-2" style="text-align: center;">Join Us!</h2>
            <p style="text-align: center; font-size: 20px; line-height: 1.5; margin: 20px 100px; text-align: justify;">
                <b>EgoLife</b> is an evolving initiative that aims to push the boundaries of egocentric AI. We are actively working to enhance every aspect of the project - from <span style="color: #4CAF50;">expanding our dataset</span> and <span style="color: #FF9800;">enriching annotations</span> to <span style="color: #2196F3;">advancing our omnimodal models</span> and <span style="color: #9C27B0;">refining the long-range system II architecture</span>. We warmly welcome researchers who share our vision to join this exciting journey. If you're interested in contributing to the future of egocentric AI assistants, please reach out to us at <a href="mailto:jingkang001@e.ntu.edu.sg" style="color: #E91E63;">jingkang001@e.ntu.edu.sg</a>. Together, let's bring truly personalized AI assistance into reality!
            </p>
        </div>
    </div>
</section>



<!-- Youtube video
<section class="hero is-small is-light">
    <div class="hero-body">
        <div class="container">
            <h2 class="title is-3">Video Presentation</h2>
            <div class="columns is-centered has-text-centered">
                <div class="column is-four-fifths">

                    <div class="publication-video">
                        <iframe src="https://www.youtube.com/embed/JkaxUblCGz0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
                    </div>
                </div>
            </div>
        </div>
    </div>
</section>
End youtube video -->


<!-- Video carousel
<section class="hero is-small">
    <div class="hero-body">
        <div class="container">
            <h2 class="title is-3">Another Carousel</h2>
            <div id="results-carousel" class="carousel results-carousel">
                <div class="item item-video1">
                    <video poster="" id="video1" autoplay controls muted loop height="100%">
        <source src="static/videos/carousel1.mp4"
        type="video/mp4">
      </video>
                </div>
                <div class="item item-video2">
                    <video poster="" id="video2" autoplay controls muted loop height="100%">
        <source src="static/videos/carousel2.mp4"
        type="video/mp4">
      </video>
                </div>
                <div class="item item-video3">
                    <video poster="" id="video3" autoplay controls muted loop height="100%">
        <source src="static/videos/carousel3.mp4"
        type="video/mp4">
      </video>
                </div>
            </div>
        </div>
    </div>
</section>
End video carousel -->






<!-- Paper poster -->
<!-- <section class="hero is-small is-light">
    <div class="hero-body">
        <div class="container">
            <h2 class="title" style="text-align: center;">Paper</h2>

            <iframe src="static/pdfs/FunQA.pdf" width="100%" height="550">
            </iframe>

        </div>
    </div>
</section> -->
<!--End paper poster -->


<!--BibTex citation -->
<section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
        <h2 class="title">BibTeX</h2>
        <pre><code>@inproceedings{yang2025egolife,
  title={EgoLife: Towards Egocentric Life Assistant},
  author={Yang, Jingkang and Liu, Shuai and Guo, Hongming and Dong, Yuhao and Zhang, Xiamengwei and Zhang, Sicheng and Wang, Pengyun and Zhou, Zitang and Xie, Binzhu and Wang, Ziyue and Ouyang, Bei and Lin, Zhengyu and Cominelli, Marco and Cai, Zhongang and Zhang, Yuanhan and Zhang, Peiyuan and Hong, Fangzhou and Widmer, Joerg and Gringoli, Francesco and Yang, Lei and Li, Bo and Liu, Ziwei},
  booktitle={The IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  year={2025},
}</code></pre>
    </div>
</section>
<!--End BibTex citation -->


<footer class="footer">
    <div class="container">
        <div class="columns is-centered">
            <div class="column is-8">
                <div class="content">

                    <p>
                        This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a>. <br> This website is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/"
                                                                                                                                                                                                                             target="_blank">Creative
                        Commons Attribution-ShareAlike 4.0 International License</a>.
                    </p>

                </div>
            </div>
        </div>
    </div>
</footer>

<!-- Statcounter tracking code -->

<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

<!-- End of Statcounter Code -->

</body>

</html>
